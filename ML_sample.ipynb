{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxgAWAV7sIbo",
        "outputId": "1cb32a21-6ae2-4588-be8a-579f415c2331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 9,599 kB of archives.\n",
            "After this operation, 29.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 fonts-nanum all 20180306-3 [9,599 kB]\n",
            "Fetched 9,599 kB in 2s (4,164 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 122349 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20180306-3_all.deb ...\n",
            "Unpacking fonts-nanum (20180306-3) ...\n",
            "Setting up fonts-nanum (20180306-3) ...\n",
            "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "# 폰트 다운로드 (런타임 재부팅 해야함)\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot 라이브러리\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# 경고 메시지 무시\n",
        "import warnings\n",
        "# 실행결과 경고메시지 출력 제외\n",
        "warnings.filterwarnings('ignore')\n",
        "# 글꼴 설정\n",
        "plt.rc('font', family='NanumBarunGothic') "
      ],
      "metadata": {
        "id": "PsuMPrYqsiFb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ALL => 개 개  고양이 고양이 고양이 고양이 고양이 고양이 고양이\n",
        "\n",
        "* 예를들면 극단적으로 데이터가 개는 앞쪽에 쏠리고 고양이는 뒷쪽에 쏠릴경우\n",
        "  학습 할 수록 개를 학습한 내용이 희미해지고 고양이에 쏠리게됨\n",
        "  "
      ],
      "metadata": {
        "id": "12g0ou2QvkDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 split하기 위한 도구\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 고차원 화 \n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# 데이터 rescaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 사이킷런 전처리 파이프라인 만들기 위해\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# 행렬 연산 라이브러리\n",
        "import numpy as np\n",
        "\n",
        "# 사이킷런의 linear model 중 stochastic gradient descent \n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "# 사이킷런의 loss function 중 하나인 mean squared error 모듈 (MSE)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 시스템 접근 모듈\n",
        "import os\n",
        "\n",
        "# random 기준\n",
        "np.random.seed(42)\n",
        "\n",
        "# 그림을 저장할 폴더\n",
        "PROJECT_ROOT_DIR = '.'\n",
        "\n",
        "# 100개 샘플 예정\n",
        "m=100\n",
        "\n",
        "# 100개의 샘플 사용 데이터\n",
        "X = 6 * np.random.rand(m, 1)-3\n",
        "\n",
        "# 100개의 샘플 사용 데이터의 정답값\n",
        "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
        "\n",
        "# train set, validation set                     # ravel 다차원을 1차원으로,\n",
        "                                                # test_size = validation set 50%, random_state suffling 기준\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state =10)\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(X_val))\n",
        "print(len(y_train))\n",
        "print(len(y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfOenZ5Rsi4K",
        "outputId": "e81bcaa3-eecb-481e-b5d3-2a9faf0e960d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n",
            "25\n",
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 파이프라인 polynomial Features의 차수는 2\n",
        "poly_scaler = Pipeline([\n",
        "    (\"poly_features\", PolynomialFeatures(degree = 2, include_bias=False)),\n",
        "    (\"std_scaler\", StandardScaler()),\n",
        "])\n",
        "\n",
        "# X로 fitting 한 다음에 변환까지\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "\n",
        "# 이미 X_train으로 poly_scaler 를 fitting 해서 변환만 해줌 / 데이터가 상대적으로 많은 X_train으로 fitting 시킴\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "\n",
        "# SGDRegressor 모델 정의\n",
        "# penalty: {'l2', 'l1', 'elasticnet'}, 기본값='l2' 사용할 페널티(정규화 용어라고도 함)입니다. 선형 SVM 모델의 표준 정규화 장치인 'l2'가 기본값입니다. 'l1' 및 'elasticnet'은 'l2'로 달성할 수 없는 모델(기능 선택)에 희소성을 가져올 수 있습니다.\n",
        "# eta0 float, 기본값=0.01 초기 학습률입니다. 기본값은 0.01입니다.\n",
        "# warm_start의 default는 False임. 이는 .fit을 실행할 때, 이전에 업데이트된 weight(coefficient)를 초기화하고 다시 fitting한다는 것을 의미한다. 반대로 True는 이전 호출에 사용했던 solution을 재사용 할지 여부 결정\n",
        "# learning_rate=\"constant\" : Learning Rate로 지정한 상수값을 계속 사용하는 것을 나타냅니다.\n",
        "sgd_reg = SGDRegressor(max_iter=1,                 # 횟수 설정, 여기에 500을 넣지 않는 이유는 max_iter가 없는 모델들도 있어서 일단 체험\n",
        "                       penalty=None,               #\n",
        "                       eta0=0.0005,                # learning rate 수치\n",
        "                       warm_start=True,            # ★\n",
        "                       learning_rate=\"constant\",   # \n",
        "                       random_state=42)            #\n",
        "\n",
        "# 학습 횟수\n",
        "n_epochs = 500\n",
        "\n",
        "# 학습 오차와 validation 오차를 담을 리스트 그릇 선언\n",
        "train_errors, val_errors = [], []\n",
        "\n",
        "# 학습 반복 (500번)\n",
        "for epoch in range(n_epochs):\n",
        "  # fitting, polynomial 시킨거랑 정답값으로 fitting,\n",
        "  sgd_reg.fit(X_train_poly_scaled, y_train)\n",
        "  \n",
        "  # fitting된 모델에 train X와 validation X 넣어서 각각 예측값 도출                  # 반복문 루프마다 fitting 시키고, 곧 바로 predict를 시키고                          //  over fitting\n",
        "  y_train_predict = sgd_reg.predict(X_train_poly_scaled)                             # MSE를 만들고, 리스트에 넣어서 몇번째 반복학습할때 에러가 제일 적은지 확인\n",
        "  y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "\n",
        "  # MSE 구한 후 리스트에 각각 담기\n",
        "  train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
        "  val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "# argmin에서 몇번째에 val error가 낮냐? 즉 몇번째가 몇번째 epoch냐\n",
        "best_epoch = np.argmin(val_errors)\n",
        "\n",
        "# best epoch에서 val_error를 찾고, 그 값은 MSE라서 RMSE로 구함.\n",
        "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
        "\n"
      ],
      "metadata": {
        "id": "IHI8heC0wxZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import clone\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1, warm_start=True, penalty=None,\n",
        "                       learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # 이어서 학습합니다\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    if val_error < minimum_val_error:\n",
        "        count=0\n",
        "        minimum_val_error = val_error\n",
        "        best_epoch = epoch\n",
        "        best_model = clone(sgd_reg)\n",
        "    else: # 만약에 minimum error 가 연속으로 뜬다면 break 시킬거임\n",
        "      count += 1\n",
        "      if count==5:\n",
        "        break         # early stopping (조기종료)"
      ],
      "metadata": {
        "id": "eCwMsC2e7WXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 파이프라인 polynomial Features의 차수는 90\n",
        "poly_scaler = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "        (\"std_scaler\", StandardScaler()),\n",
        "    ])\n",
        "\n",
        "# X로 fitting 한다음에 변환까지\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "\n",
        "# 이미 X_train으로 poly_scaler를 fitting해서 변환만 해줌.\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "# SGDRegressor 모델 정의\n",
        "\n",
        "\n",
        "# penalty: {'l2', 'l1', 'elasticnet'}, 기본값='l2' 사용할 페널티(정규화 용어라고도 함)입니다. 선형 SVM 모델의 표준 정규화 장치인 'l2'가 기본값입니다. 'l1' 및 'elasticnet'은 'l2'로 달성할 수 없는 모델(기능 선택)에 희소성을 가져올 수 있습니다.\n",
        "# eta0 float, 기본값=0.01 초기 학습률입니다. 기본값은 0.01입니다.\n",
        "# warm_start의 default는 False임. 이는 .fit을 실행할 때, 이전에 업데이트된 weight(coefficient)를 초기화하고 다시 fitting한다는 것을 의미한다. 반대로 True는 이전 호출에 사용했던 solution을 재사용 할지 여부 결정\n",
        "# learning_rate=\"constant\" : Learning Rate로 지정한 상수값을 계속 사용하는 것을 나타냅니다.\n",
        "sgd_reg = SGDRegressor(max_iter=1,\n",
        "                       penalty=None,\n",
        "                       eta0=0.0005,\n",
        "                       warm_start=True,\n",
        "                       learning_rate=\"constant\",\n",
        "                       random_state=42)\n",
        "\n",
        "# 학습 횟수\n",
        "n_epochs = 500\n",
        "\n",
        "# 학습오차와 validation 오차 담을 리스트 그릇\n",
        "train_errors, val_errors = [], []\n",
        "\n",
        "# 학습 반복\n",
        "for epoch in range(n_epochs):\n",
        "    # fitting, polynomial 시킨거랑 정답값으로 fitting.\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
        "\n",
        "    # fitting된 모델에 train X와 validation X 넣어서 각각 예측값 도출\n",
        "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "\n",
        "    # MSE 구한 후 리스트에 각각 담기\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
        "    val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "# argmin에서 몇번째에 val error가 낮냐? 즉 몇번째가 몇번째 epoch냐\n",
        "best_epoch = np.argmin(val_errors)\n",
        "\n",
        "# best epoch에서 val_error를 찾고, 그 값은 MSE라서 RMSE로 구함.\n",
        "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
        "\n",
        "# plot에서 설정할 값들.\n",
        "# xytext : 텍스트 위치 \n",
        "# xy : 화살표 위치\n",
        "# ha : horizontal alignment\n",
        "# arrowprops : 화살표 속성들\n",
        "\n",
        "plt.annotate('최선의 모델',\n",
        "             xy=(best_epoch, best_val_rmse),\n",
        "             xytext=(best_epoch, best_val_rmse + 1),\n",
        "             ha=\"center\",\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "             fontsize=16,\n",
        "            )\n",
        "\n",
        "best_val_rmse -= 0.03  # 그래프를 더 보기 좋게 만들기 위해\n",
        "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
        "\n",
        "# val_errors와 train_errors의 RMSE\n",
        "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"검증 세트\")\n",
        "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"훈련 세트\")\n",
        "\n",
        "# 각 화살표 네이밍\n",
        "plt.legend(loc=\"upper right\", fontsize=14)\n",
        "\n",
        "# xlabel\n",
        "plt.xlabel(\"에포크\", fontsize=14)\n",
        "\n",
        "# ylabel\n",
        "plt.ylabel(\"RMSE\", fontsize=14)\n",
        "save_fig(\"early_stopping_plot\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NZLHj-Evwlfh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}